{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing  - Jupyter Notebook\n",
    "### Cecilia, Conor, Francesco \n",
    "December 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice: create a new python virtual environemnt and run the setup.py file provided in the project repository. Then run the notebook using the virtual environment. This should install everything required.\n",
    "\n",
    "This project requires the modules found in the Requirements.txt file, which can be installed directly onto the current python installation via eg pip install -r Requirements.txt. It also requires the spacy model \"en_core_web_sm\" which can be installed via python -m spacy download en_core_web_sm after spacy in installed.\n",
    "\n",
    "Anaconda installation may differ. In particular \"conda forge install textacy\" install of \"pip install textacy\".\n",
    "\n",
    "This script should be run from the root of the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gensim.models import word2vec\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import snowballstemmer\n",
    "import spacy\n",
    "from string import ascii_lowercase\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project also requires a jdk installation. Below are examples for MacOS and Windows. Edit this cell with a path to your jdk installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your username in here. I've put the paths you wrote as an attribute in the dictionary below\n",
    "user = 'Conor'\n",
    "\n",
    "# Dictionary containing the Java paths for each user\n",
    "java_paths = {\n",
    "    'Conor': '/usr/lib/jvm/java-8-openjdk-amd64',\n",
    "    'Francesco': '/Users/macbookpro/Downloads/jdk-13.0.1.jdk/Contents/Home/bin/java',\n",
    "    'Cecilia': 'C:/Program Files/Java/jdk-13.0.1/bin/java.exe'\n",
    "}\n",
    "\n",
    "java_path = java_paths[user]\n",
    "\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>​ UDAY states see Rs 11,989-crore drop in inte...</td>\n",
       "      <td>20/03/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Income Tax India: 1 taxpayer owes 11% of India...</td>\n",
       "      <td>24/01/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$10 billion unhedged gap in foreign exchange d...</td>\n",
       "      <td>17/10/16</td>\n",
       "      <td>Economic Times India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10% cost reduction can add $5.5 billion to Ind...</td>\n",
       "      <td>19/01/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>​10% ethanol blending can save $1.7 billion in...</td>\n",
       "      <td>11/08/15</td>\n",
       "      <td>Economic Times India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65587</th>\n",
       "      <td>67548</td>\n",
       "      <td>Sir Sandy Crombie to join RBS board | The Inde...</td>\n",
       "      <td>23/05/09</td>\n",
       "      <td>The Independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65588</th>\n",
       "      <td>67549</td>\n",
       "      <td>'We had a lot of fun': Sir Stephen Nickell ref...</td>\n",
       "      <td>25/01/17</td>\n",
       "      <td>The Independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65589</th>\n",
       "      <td>67550</td>\n",
       "      <td>Sir Tom calls time on Dobbies interest | The I...</td>\n",
       "      <td>21/05/08</td>\n",
       "      <td>The Independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65590</th>\n",
       "      <td>67551</td>\n",
       "      <td>Sir Victor Blank joins list for M&amp;amp;S chairm...</td>\n",
       "      <td>16/05/10</td>\n",
       "      <td>The Independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65591</th>\n",
       "      <td>67552</td>\n",
       "      <td>Sirius Minerals: Future of Yorkshire employer ...</td>\n",
       "      <td>17/09/19</td>\n",
       "      <td>The Independent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62526 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           Headline  \\\n",
       "0               0  ​ UDAY states see Rs 11,989-crore drop in inte...   \n",
       "1               1  Income Tax India: 1 taxpayer owes 11% of India...   \n",
       "2               2  $10 billion unhedged gap in foreign exchange d...   \n",
       "3               3  10% cost reduction can add $5.5 billion to Ind...   \n",
       "5               5  ​10% ethanol blending can save $1.7 billion in...   \n",
       "...           ...                                                ...   \n",
       "65587       67548  Sir Sandy Crombie to join RBS board | The Inde...   \n",
       "65588       67549  'We had a lot of fun': Sir Stephen Nickell ref...   \n",
       "65589       67550  Sir Tom calls time on Dobbies interest | The I...   \n",
       "65590       67551  Sir Victor Blank joins list for M&amp;S chairm...   \n",
       "65591       67552  Sirius Minerals: Future of Yorkshire employer ...   \n",
       "\n",
       "           Date              Provider  \n",
       "0      20/03/17  Economic Times India  \n",
       "1      24/01/17  Economic Times India  \n",
       "2      17/10/16  Economic Times India  \n",
       "3      19/01/17  Economic Times India  \n",
       "5      11/08/15  Economic Times India  \n",
       "...         ...                   ...  \n",
       "65587  23/05/09       The Independent  \n",
       "65588  25/01/17       The Independent  \n",
       "65589  21/05/08       The Independent  \n",
       "65590  16/05/10       The Independent  \n",
       "65591  17/09/19       The Independent  \n",
       "\n",
       "[62526 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = pd.read_json('data/word_vectors/SP500_Tickers.json', typ='series')\n",
    "\n",
    "header = pd.read_csv('headline_scraping/full_headlines.csv', sep = \",\")\n",
    "header = header.drop_duplicates('Headline')\n",
    "header[(header['Date']=='31/10/19') & (header['Headline'].str.contains('goods rivals'))].loc[41725,'Headline']\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of words to not include in headlines (improves SVO extraction accuracy)\n",
    "stemmer = snowballstemmer.EnglishStemmer()\n",
    "stop = stopwords.words('english')\n",
    "stoplist = stemmer.stemWords(stop)\n",
    "stoplist = set(stoplist)\n",
    "stop = set(sorted(stop + list(stoplist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove characters and stoplist words, then generate dictionary of unique words\n",
    "data = header\n",
    "\n",
    "data['Original_Headline'] = data['Headline']\n",
    "data['Headline'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\$&]','',inplace=True,regex=True)\n",
    "\n",
    "extensions_list = ['reuters','Reuters','bloomberg', 'may','also','could','would', 'na','&amp','gets', 'getting', 'get','must','might','may','across','among','beside','however','yet','within']+list(ascii_lowercase)\n",
    "\n",
    "stop = stop.union(set(extensions_list))\n",
    "\n",
    "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*data['Headline'].str.split(' ')))))).split(\" \"))\n",
    "\n",
    "data['Headline'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in data['Headline'].str.lower().str.split(' ')]\n",
    "\n",
    "header = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGILENT TECHNOLOGIES</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMERICAN AIRLINES</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADVANCE AUTO PARTS</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPLE</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBVIE</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>XYLEM</td>\n",
       "      <td>XYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>YUM! BRANDS</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ZIMMER BIOMET HOLDINGS</td>\n",
       "      <td>ZBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>ZIONS BANCORPORATION NA</td>\n",
       "      <td>ZION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZOETIS</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name Ticker\n",
       "0       AGILENT TECHNOLOGIES      A\n",
       "1          AMERICAN AIRLINES    AAL\n",
       "2         ADVANCE AUTO PARTS    AAP\n",
       "3                      APPLE   AAPL\n",
       "4                     ABBVIE   ABBV\n",
       "..                       ...    ...\n",
       "495                    XYLEM    XYL\n",
       "496              YUM! BRANDS    YUM\n",
       "497   ZIMMER BIOMET HOLDINGS    ZBH\n",
       "498  ZIONS BANCORPORATION NA   ZION\n",
       "499                   ZOETIS    ZTS\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = tickers.to_frame('Ticker').reset_index()\n",
    "ticker.columns=['Name','Ticker']\n",
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a|aal|aap|aapl|abbv|abc|abmd|abt|acn|adbe|adi|adm|adp|ads|adsk|aee|aep|aes|afl|agn|aig|aiv|aiz|ajg|akam|alb|algn|alk|all|alle|alxn|amat|amcr|amd|ame|amg|amgn|amp|amt|amzn|anet|anss|antm|aon|aos|apa|apd|aph|aptv|are|arnc|ato|atvi|avb|avgo|avy|awk|axp|azo|ba|bac|bax|bbt|bby|bdx|ben|bf.b|bhge|biib|bk|bkng|blk|bll|bmy|br|brk.b|bsx|bwa|bxp|c|cag|cah|cat|cb|cboe|cbre|cbs|cci|ccl|cdns|cdw|ce|celg|cern|cf|cfg|chd|chrw|chtr|ci|cinf|cl|clx|cma|cmcsa|cme|cmg|cmi|cms|cnc|cnp|cof|cog|coo|cop|cost|coty|cpb|cpri|cprt|crm|csco|csx|ctas|ctl|ctsh|ctva|ctxs|cvs|cvx|cxo|d|dal|dd|de|dfs|dg|dgx|dhi|dhr|dis|disck|dish|dlr|dltr|dov|dow|dre|dri|dte|duk|dva|dvn|dxc|ea|ebay|ecl|ed|efx|eix|el|emn|emr|eog|eqix|eqr|es|ess|etfc|etn|etr|evrg|ew|exc|expd|expe|exr|f|fang|fast|fb|fbhs|fcx|fdx|fe|ffiv|fis|fisv|fitb|flir|fls|flt|fmc|foxa|frc|frt|fti|ftnt|ftv|gd|ge|gild|gis|gl|glw|gm|googl|gpc|gpn|gps|grmn|gs|gww|hal|has|hban|hbi|hca|hcp|hd|hes|hfc|hig|hii|hlt|hog|holx|hon|hp|hpe|hpq|hrb|hrl|hsic|hst|hsy|hum|ibm|ice|idxx|iex|iff|ilmn|incy|info|intc|intu|ip|ipg|ipgp|iqv|ir|irm|isrg|it|itw|ivz|jbht|jci|jec|jkhy|jnj|jnpr|jpm|jwn|k|key|keys|khc|kim|klac|kmb|kmi|kmx|ko|kr|kss|ksu|l|lb|ldos|leg|len|lh|lhx|lin|lkq|lly|lmt|lnc|lnt|low|lrcx|luv|lvs|lw|lyb|m|ma|maa|mac|mar|mas|mcd|mchp|mck|mco|mdlz|mdt|met|mgm|mhk|mkc|mktx|mlm|mmc|mmm|mnst|mo|mos|mpc|mrk|mro|ms|msci|msft|msi|mtb|mtd|mu|mxim|myl|nbl|nclh|ndaq|nee|nem|nflx|ni|nke|nlsn|noc|nov|nrg|nsc|ntap|ntrs|nue|nvda|nvr|nwl|nwsa|o|oke|omc|orcl|orly|oxy|payx|pbct|pcar|peg|pep|pfe|pfg|pg|pgr|ph|phm|pkg|pki|pld|pm|pnc|pnr|pnw|ppg|ppl|prgo|pru|psa|psx|pvh|pwr|pxd|pypl|qcom|qrvo|rcl|re|reg|regn|rf|rhi|rjf|rl|rmd|rok|rol|rop|rost|rsg|rtn|sbac|sbux|schw|see|shw|sivb|sjm|slb|slg|sna|snps|so|spg|spgi|sre|sti|stt|stx|stz|swk|swks|syf|syk|symc|syy|t|tap|tdg|tel|tfx|tgt|tif|tjx|tmo|tmus|tpr|trip|trow|trv|tsco|tsn|ttwo|twtr|txn|txt|uaa|ual|udr|uhs|ulta|unh|unm|unp|ups|uri|usb|utx|v|var|vfc|viab|vlo|vmc|vno|vrsk|vrsn|vrtx|vtr|vz|wab|wat|wba|wcg|wdc|wec|well|wfc|whr|wltw|wm|wmb|wmt|wrk|wu|wy|wynn|xec|xel|xlnx|xom|xray|xrx|xyl|yum|zbh|zion|zts\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the words/tickers of the company's names in the S&P500 \n",
    "#In order to select only the headlines that concern the S&P500 companies. \n",
    "\n",
    "company_names = ticker['Name'].to_list()\n",
    "ticker_list = ticker['Ticker'].to_list()\n",
    "\n",
    "company_regex = str1 = '|'.join(ticker_list).lower()\n",
    "company_regex = company_regex.replace( '&', ' ',)\n",
    "company_regex = company_regex.replace( '   ', '&',)\n",
    "company_regex = company_regex.replace( '  ', '&',)\n",
    "company_regex = company_regex.replace( ' ', '&',)\n",
    "company_regex = company_regex.replace( '-', '&',)\n",
    "\n",
    "print(company_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65587</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65588</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65589</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65590</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65591</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62526 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Headline\n",
       "0          True\n",
       "1          True\n",
       "2          True\n",
       "3          True\n",
       "5          True\n",
       "...         ...\n",
       "65587      True\n",
       "65588      True\n",
       "65589      True\n",
       "65590      True\n",
       "65591      True\n",
       "\n",
       "[62526 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Continue only with headlines which contain a company name or ticker\n",
    "valid = pd.DataFrame(header['Headline'].str.contains(company_regex, regex=True))\n",
    "valid\n",
    "# len(good_indexes)\n",
    "# header = header[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Provider</th>\n",
       "      <th>Original_Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>​ uday states see rs crore drop interest cost ...</td>\n",
       "      <td>20/03/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "      <td>​ UDAY states see Rs 11,989-crore drop in inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>income tax india taxpayer owes indias individu...</td>\n",
       "      <td>24/01/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "      <td>Income Tax India: 1 taxpayer owes 11% of India...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>billion unhedged gap foreign exchange deposit ...</td>\n",
       "      <td>17/10/16</td>\n",
       "      <td>Economic Times India</td>\n",
       "      <td>$10 billion unhedged gap in foreign exchange d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>cost reduction add billion indias trade revenu...</td>\n",
       "      <td>19/01/17</td>\n",
       "      <td>Economic Times India</td>\n",
       "      <td>10% cost reduction can add $5.5 billion to Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>​ ethanol blending save billion forex india re...</td>\n",
       "      <td>11/08/15</td>\n",
       "      <td>Economic Times India</td>\n",
       "      <td>​10% ethanol blending can save $1.7 billion in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62518</th>\n",
       "      <td>65587</td>\n",
       "      <td>67548</td>\n",
       "      <td>sir sandy crombie join rbs board independent</td>\n",
       "      <td>23/05/09</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>Sir Sandy Crombie to join RBS board | The Inde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62519</th>\n",
       "      <td>65588</td>\n",
       "      <td>67549</td>\n",
       "      <td>lot fun sir stephen nickell reflects austerity...</td>\n",
       "      <td>25/01/17</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>'We had a lot of fun': Sir Stephen Nickell ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62520</th>\n",
       "      <td>65589</td>\n",
       "      <td>67550</td>\n",
       "      <td>sir tom calls time dobbies interest independent</td>\n",
       "      <td>21/05/08</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>Sir Tom calls time on Dobbies interest | The I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62521</th>\n",
       "      <td>65590</td>\n",
       "      <td>67551</td>\n",
       "      <td>sir victor blank joins list mamps chairman ind...</td>\n",
       "      <td>16/05/10</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>Sir Victor Blank joins list for M&amp;amp;S chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62522</th>\n",
       "      <td>65591</td>\n",
       "      <td>67552</td>\n",
       "      <td>sirius minerals future yorkshire employer ques...</td>\n",
       "      <td>17/09/19</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>Sirius Minerals: Future of Yorkshire employer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62523 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  Unnamed: 0                                           Headline  \\\n",
       "0          0           0  ​ uday states see rs crore drop interest cost ...   \n",
       "1          1           1  income tax india taxpayer owes indias individu...   \n",
       "2          2           2  billion unhedged gap foreign exchange deposit ...   \n",
       "3          3           3  cost reduction add billion indias trade revenu...   \n",
       "4          5           5  ​ ethanol blending save billion forex india re...   \n",
       "...      ...         ...                                                ...   \n",
       "62518  65587       67548       sir sandy crombie join rbs board independent   \n",
       "62519  65588       67549  lot fun sir stephen nickell reflects austerity...   \n",
       "62520  65589       67550    sir tom calls time dobbies interest independent   \n",
       "62521  65590       67551  sir victor blank joins list mamps chairman ind...   \n",
       "62522  65591       67552  sirius minerals future yorkshire employer ques...   \n",
       "\n",
       "           Date              Provider  \\\n",
       "0      20/03/17  Economic Times India   \n",
       "1      24/01/17  Economic Times India   \n",
       "2      17/10/16  Economic Times India   \n",
       "3      19/01/17  Economic Times India   \n",
       "4      11/08/15  Economic Times India   \n",
       "...         ...                   ...   \n",
       "62518  23/05/09       The Independent   \n",
       "62519  25/01/17       The Independent   \n",
       "62520  21/05/08       The Independent   \n",
       "62521  16/05/10       The Independent   \n",
       "62522  17/09/19       The Independent   \n",
       "\n",
       "                                       Original_Headline  \n",
       "0      ​ UDAY states see Rs 11,989-crore drop in inte...  \n",
       "1      Income Tax India: 1 taxpayer owes 11% of India...  \n",
       "2      $10 billion unhedged gap in foreign exchange d...  \n",
       "3      10% cost reduction can add $5.5 billion to Ind...  \n",
       "4      ​10% ethanol blending can save $1.7 billion in...  \n",
       "...                                                  ...  \n",
       "62518  Sir Sandy Crombie to join RBS board | The Inde...  \n",
       "62519  'We had a lot of fun': Sir Stephen Nickell ref...  \n",
       "62520  Sir Tom calls time on Dobbies interest | The I...  \n",
       "62521  Sir Victor Blank joins list for M&amp;S chairm...  \n",
       "62522  Sirius Minerals: Future of Yorkshire employer ...  \n",
       "\n",
       "[62523 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_indices = header[valid['Headline'] == True].reset_index()['index']\n",
    "header = header.loc[valid_indices,:].reset_index()#[['Date','Header']]\n",
    "# index\n",
    "\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVO and Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy en_core_web_sm nlp model for SVO extraction\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "st = StanfordNERTagger(\n",
    "    'ner/stanford-ner-2014-06-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "    'ner/stanford-ner-2014-06-16/stanford-ner.jar',\n",
    "    encoding = 'utf-8'\n",
    ")\n",
    "COMPANY_TYPES = ['PERSON', 'ORGANIZATION']\n",
    "\n",
    "def get_ticker(name, tickers):\n",
    "    for key, val in tickers.items():\n",
    "        # Check if this is a ticker\n",
    "        if val.upper() == name.upper():\n",
    "            return f'__{val}'\n",
    "        \n",
    "        # Check that this is \n",
    "        if name.upper() in key.upper():\n",
    "            return f'__{val}'\n",
    "    return None\n",
    "\n",
    "def parse_header(header,tickers,orig):\n",
    "    '''Attempt to replace all organisations in a header with their ticker'''\n",
    "    header = header.lower()\n",
    "    nlp_header = nlp(header)\n",
    "\n",
    "    tokens = list(textacy.extract.subject_verb_object_triples(nlp_header))  \n",
    "\n",
    "    parsed_words = []\n",
    "    for i in range(len(tokens)):\n",
    "        ticker_0 = get_ticker(str(tokens[i][0]), tickers)\n",
    "        ticker_2 = get_ticker(str(tokens[i][2]), tickers)  \n",
    "        sub = str(tokens[i][0])\n",
    "        obj = str(tokens[i][2])\n",
    "        tick_sub = ''\n",
    "        tick_obj = ''\n",
    "        if ticker_0 is not None:\n",
    "            sub = ticker_0\n",
    "            tick_sub = 'sub'\n",
    "        if ticker_2 is not None:\n",
    "            obj = ticker_2\n",
    "            tick_obj = 'obj'\n",
    "        verb = str(tokens[i][1])\n",
    "        tick = tick_sub + tick_obj\n",
    "        \n",
    "        if (ticker_0 != None) | (ticker_2 != None):\n",
    "            parsed_words.append([sub, verb, obj, tick, header,orig])\n",
    "        \n",
    "    return parsed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_df = pd.DataFrame(columns = ['Date', 'Sub', 'Verb', 'Obj', 'Tick_Type', 'Headlines', 'Original Headline'])\n",
    "\n",
    "for i in range (len(header['Headline'])):\n",
    "    #print(i)\n",
    "    headline = header.loc[i,'Headline']\n",
    "    orig_head = header.loc[i,'Original_Headline'] \n",
    "    date = header.loc[i,'Date']\n",
    "    svo_headlines = parse_header(headline,tickers,orig_head)\n",
    "    \n",
    "    for ls in svo_headlines:\n",
    "        svo_df.loc[len(svo_df)] =  [date] + ls\n",
    "    \n",
    "svo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = svo_df\n",
    "svo_df['Date'] = pd.to_datetime(svo_df['Date'])\n",
    "\n",
    "svo_df = svo_df.sort_values(by='Date')\n",
    "\n",
    "length_training = int(np.floor(len(svo_df)*80/100))\n",
    "date = svo_df['Date'].values\n",
    "critical_date = date[length_training]\n",
    "print(critical_date)\n",
    "# print([x for x in svo_df['Verb'] if len(x.split(' ')) > 1])\n",
    "# length_training = int(np.floor(len(svo_df)*80/100))\n",
    "training = svo_df[svo_df['Date']<=critical_date]\n",
    "test_data = svo_df[svo_df['Date']>critical_date]\n",
    "training\n",
    "# print((training.index))\n",
    "# svo_df\n",
    "#training.loc[40,'Obj']\n",
    "\n",
    "#print(type(test.loc[1,'Date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=word2vec.Word2Vec.load('word_vectors/models/model_1')\n",
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 100\n",
    "kmeans = cluster.KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create phrases vectors\n",
    "#### (Concatenating the vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "to_parse = {\n",
    "    'obj': 'Sub',\n",
    "    'sub': 'Obj',\n",
    "    'subobj': 'Verb'\n",
    "}\n",
    "\n",
    "for i in training.index:\n",
    "    entry = {}\n",
    "    for key in ['Date', 'Sub', 'Verb','Obj', 'Tick_Type', 'Headlines', 'Original Headline']:\n",
    "        entry[key] = training.loc[i, key]\n",
    "        \n",
    "#     print(entry['Verb'])\n",
    "    \n",
    "    try:\n",
    "        verb_vector = model.wv[entry['Verb'].upper()]\n",
    "    except:\n",
    "        verb_vector = None\n",
    "    key_to_parse = to_parse[entry['Tick_Type']]\n",
    "    \n",
    "    try:\n",
    "        other_vector = model.wv[entry[key_to_parse].upper()]\n",
    "    except:\n",
    "        other_vector = None\n",
    "        \n",
    "    if verb_vector is not None and other_vector is not None:\n",
    "        final_vector = np.concatenate((verb_vector, other_vector))\n",
    "        entry['final_vector'] = final_vector\n",
    "    else:\n",
    "        entry['final_vector'] = None\n",
    "    \n",
    "    data.append(entry)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which tickers are the most quoted by articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(data)\n",
    "tickers_list = []\n",
    "for x in data:\n",
    "    if (x['Tick_Type'] == 'sub')&(x['final_vector'] is not None):\n",
    "        tickers_list.append(x['Sub'])\n",
    "    elif (x['Tick_Type'] == 'obj')&(x['final_vector'] is not None):\n",
    "        tickers_list.append(x['Obj'])\n",
    "\n",
    "#print(tickers_list)\n",
    "\n",
    "from collections import Counter\n",
    "counts = dict(Counter(tickers_list))\n",
    "\n",
    "to_parse = {'tickers': [], 'counts': []}\n",
    "for ticker, count in counts.items():\n",
    "    to_parse['tickers'].append(ticker)\n",
    "    to_parse['counts'].append(count)\n",
    "print(to_parse)\n",
    "    \n",
    "\n",
    "ticker_counts = pd.DataFrame(to_parse)\n",
    "ticker_counts = ticker_counts.sort_values(by = 'counts', ascending = False)\n",
    "ticker_counts1 = ticker_counts[ticker_counts['counts']>5]\n",
    "plt.figure(figsize = [20,10])\n",
    "plt.scatter(ticker_counts1['tickers'], ticker_counts1['counts'])\n",
    "plt.xticks(rotation = 'vertical')\n",
    "#print(json.dumps(counts, indent = 2))\n",
    "ticker_counts.head()\n",
    "# plt.figure(figsize=(10,5))\n",
    "# chart = sns.countplot(\n",
    "#     data = ticker_counts,\n",
    "#     x='tickers',\n",
    "#     y = 'counts',\n",
    "#     palette='Set1'\n",
    "# )\n",
    "# chart.set_xticklabels(chart.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering phrases vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = [x for x in data if x['final_vector'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = [x['final_vector'] for x in data if x['final_vector'] is not None]\n",
    "\n",
    "vocab = [x['final_vector'] for x in relevant_data]\n",
    "num_clusters = 100\n",
    "kmeans = cluster.KMeans(n_clusters = num_clusters)\n",
    "kmeans.fit(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "print(len(labels))\n",
    "# print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(relevant_data)):\n",
    "    relevant_data[i]['cluster_label'] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "y = labels\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights, algorithm = 'ball_tree')\n",
    "    clf.fit(vocab, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.fit(vocab, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = clf.kneighbors([vocab[2]],n_neighbors)\n",
    "A\n",
    "# print(type([vocab[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnsdf = pd.read_csv('data/returns/returnsSP500.csv')\n",
    "returnsdf\n",
    "\n",
    "# print(len(returnsdf.columns))\n",
    "# print('GOOG' in returnsdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(returnsdf.columns))\n",
    "#print(returnsdf.columns[22:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = pd.read_csv('data/SP500_parameters.csv')\n",
    "params['Ticker_b']=params['Ticker']\n",
    "params['Ticker']='__'+params['Ticker']\n",
    "params\n",
    "# params.loc[50:100,'Ticker_b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[params['Ticker_b']=='FLS']\n",
    "\n",
    "'FLS' in returnsdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker_counts.columns=['Ticker','count']\n",
    "ticker_counts = ticker_counts.merge(params[['Ticker','GICS']],on = 'Ticker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which industries are the most tackled by headlines \n",
    "##### in order to have enough data to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GICS_count = ticker_counts.groupby(['GICS']).agg(['sum'])\n",
    "GICS_count =GICS_count.sort_values(by = ('count','sum'),ascending=False)\n",
    "# GICS_count\n",
    "# plt.plot(GICS_count[('count','sum')])\n",
    "\n",
    "plt.figure(figsize = [20,10])\n",
    "# plt.plot(GICS_count[('count','sum')], marker = 'o')\n",
    "GICS_count[('count','sum')].plot(kind='bar')\n",
    "# plt.bar(GICS_count['count'], GICS_count['sum'])\n",
    "\n",
    "plt.xlabel('GICS Industry Classification')\n",
    "plt.ylabel('Number of Articles')\n",
    "\n",
    "# plt.xticks(rotation = 'horizontal')\n",
    "\n",
    "# GICS_count = GICS_count.sort_values(by = 'count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "industries = set(params['GICS'].values)\n",
    "\n",
    "dataframe_industry_dict = {}\n",
    "for industry in industries:\n",
    "    industry_tickers = params.loc[params['GICS'] == industry, 'Ticker']\n",
    "    industry_tickers = industry_tickers.str.replace('__','')\n",
    "    industry_tickers = industry_tickers.str.replace('.','-')\n",
    "    \n",
    "    industry_tickers = list(set(industry_tickers) & set(returnsdf.columns))\n",
    "    df_industry = returnsdf[industry_tickers + ['Date']]\n",
    "    \n",
    "    df_industry['market_exp'] = df_industry[industry_tickers].mean(axis=1)\n",
    "    df_industry = df_industry.iloc[1:,:]#.dropna()\n",
    "    cols = df_industry.columns\n",
    "    cols = cols[-2:].append(cols[:(len(cols)-2)])\n",
    "    df_industry = df_industry[cols]\n",
    "    dataframe_industry_dict[industry] = df_industry\n",
    "#print(len(healthy_ticker))\n",
    "\n",
    "#print(len(df_health.columns))\n",
    "# df_health\n",
    "dataframe_industry_dict['Financials']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBOR 3M importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libor = pd.read_excel('data/LIBOR_3M.xlsx')\n",
    "libor['LIBOR 3M'] = libor['LIBOR 3M']/100\n",
    "libor['Date'] = pd.to_datetime(libor['Date'])\n",
    "libor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for industry in industries:\n",
    "    test = dataframe_industry_dict[industry]\n",
    "    test['Date'] = pd.to_datetime(test['Date'])\n",
    "    test = test.merge(libor,on = 'Date')\n",
    "    \n",
    "    cols = test.columns\n",
    "    cols = cols[-1:].append(cols[:(len(cols)-1)])\n",
    "    test = test[cols]\n",
    "    \n",
    "    dataframe_industry_dict[industry] = test\n",
    "    \n",
    "dataframe_industry_dict['Financials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of abnormal returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abret_dict = {}\n",
    "for industry in industries:\n",
    "    df_returns = dataframe_industry_dict[industry]\n",
    "    capm = pd.DataFrame(df_returns['Date'])\n",
    "    risk_free_rate = df_returns['LIBOR 3M']\n",
    "    market_avg_returns = df_returns['market_exp']\n",
    "    \n",
    "    abret_dict[industry] = pd.DataFrame(df_returns['Date'])\n",
    "    \n",
    "    for col in df_returns.columns[3:]:\n",
    "        col = col.replace('-', '.')\n",
    "        \n",
    "        beta = params.loc[params['Ticker_b'] == col]['Beta'].values[0]\n",
    "        stock_returns = returnsdf[col.replace('.', '-')]\n",
    "        \n",
    "        capm[col] = risk_free_rate + beta*(market_avg_returns-risk_free_rate)\n",
    "        \n",
    "        abret_dict[industry][col] = stock_returns - capm[col]\n",
    "        abret_dict[industry] = abret_dict[industry].where((pd.notnull(abret_dict[industry])), None)\n",
    "        \n",
    "abret_dict['Financials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lookup function to return, for a given ticker, \n",
    "# start_date and horizon, the cumulative average returns over that period\n",
    "\n",
    "def lookup_cumul_returns(ticker, start_date, horizon):\n",
    "    '''\n",
    "    Args:\n",
    "    start_date (pd.Timestamp)\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        industry_classification = params.loc[params['Ticker_b'] == ticker, 'GICS'].values[0]\n",
    "    except:\n",
    "        print(ticker)\n",
    "        return None\n",
    "        \n",
    "    relevant_df = abret_dict[industry_classification]\n",
    "    \n",
    "    start_index_values = relevant_df.loc[pd.to_datetime(relevant_df['Date']) >= pd.Timestamp(start_date),:].index.values\n",
    "    \n",
    "    if len(start_index_values) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        start_index = start_index_values[0]\n",
    "\n",
    "    end_index = start_index + horizon\n",
    "    \n",
    "    if end_index > len(relevant_df):\n",
    "        return None\n",
    "    \n",
    "    relevant_returns = relevant_df.loc[start_index + 1:end_index, ticker]\n",
    "    \n",
    "    to_return = relevant_returns.cumsum().values[-1] / horizon\n",
    "    \n",
    "    if np.isnan(to_return):\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return to_return\n",
    "    \n",
    "output = lookup_cumul_returns('A', '2010-01-04', 3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "horizons = [3, 5, 10]\n",
    "\n",
    "print(list(relevant_data[0].keys()))\n",
    "\n",
    "# Add a column with the referenced ticker for each headline\n",
    "for i, el in enumerate(relevant_data):\n",
    "    lookup_col = el['Tick_Type']\n",
    "    if lookup_col in ['sub', 'obj']:\n",
    "        el['Ticker'] = el[lookup_col.capitalize()]\n",
    "    else:\n",
    "        el['Ticker'] = None\n",
    "        \n",
    "    relevant_data[i] = el\n",
    "    \n",
    "relevant_data = [x for x in relevant_data if x['Ticker'] is not None]\n",
    "    \n",
    "# {\n",
    "#     3 {\n",
    "#         'headlines': [(Sub, Verb, Obj), (), ...],\n",
    "#         '3 day': [],\n",
    "#         '5 day': []\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Set up the data structure. Each entry looks like the above\n",
    "cluster_labels = set([x['cluster_label'] for x in relevant_data])\n",
    "\n",
    "cluster_sentiments = {}\n",
    "for label in cluster_labels:\n",
    "    cluster_sentiments[label] = {\n",
    "        'svos': [],\n",
    "        'headlines': [],\n",
    "    }\n",
    "    for horizon in horizons:\n",
    "        cluster_sentiments[label]['{}_days'.format(horizon)] = []       \n",
    "        \n",
    "for el in relevant_data:\n",
    "    svo = (el['Sub'], el['Verb'], el['Obj'])\n",
    "    start_date = el['Date']\n",
    "    ticker = el['Ticker'].replace('__', '')\n",
    "    \n",
    "    cluster_label = el['cluster_label']\n",
    "    headline = el['Original Headline']\n",
    "    \n",
    "    cluster_sentiments[cluster_label]['svos'].append(svo)\n",
    "    cluster_sentiments[cluster_label]['headlines'].append(headline)\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        horizon_cumul_returns = lookup_cumul_returns(ticker, start_date, horizon)\n",
    "        cluster_sentiments[cluster_label]['{}_days'.format(horizon)].append(horizon_cumul_returns)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentiments = copy.deepcopy(cluster_sentiments)\n",
    "\n",
    "for label in cluster_sentiments:\n",
    "    for horizon in horizons:\n",
    "        entries = cluster_sentiments[label]['{}_days'.format(horizon)]\n",
    "        non_null_entries = [x for x in entries if x is not None]\n",
    "        parsed_sentiments[label]['{}_days'.format(horizon)] = np.mean(non_null_entries)\n",
    "\n",
    "# Remove any clusters which have all nan values\n",
    "all_nan_labels = []\n",
    "for label, contents in parsed_sentiments.items():\n",
    "    all_nan = True\n",
    "    for horizon in horizons:\n",
    "        if not np.isnan(contents['{}_days'.format(horizon)]):\n",
    "            all_nan = False\n",
    "            break\n",
    "            \n",
    "    if all_nan:\n",
    "        all_nan_labels.append(label)\n",
    "        \n",
    "for label in all_nan_labels:\n",
    "    del parsed_sentiments[label]\n",
    "        \n",
    "cluster_labels = list(parsed_sentiments.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for label, contents in parsed_sentiments.items():\n",
    "    print('cluster {}'.format(label))\n",
    "    for horizon in [3,5,10]:\n",
    "#         print(contents)\n",
    "        print(contents['{}_days'.format(horizon)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_days_sentiment=[]\n",
    "medium_days_sentiment = []\n",
    "large_days_sentiment = []\n",
    "for x in cluster_labels:\n",
    "    small_days_sentiment.append(1000*parsed_sentiments[x]['3_days'])\n",
    "    medium_days_sentiment.append(parsed_sentiments[x]['5_days'])\n",
    "    large_days_sentiment.append(parsed_sentiments[x]['10_days'])\n",
    "\n",
    "# fig,ax = plt.subplots(figsize = (20,4))\n",
    "fig, ax = plt.subplots()\n",
    "# fig(figsize = (20,20))\n",
    "plt.scatter(small_days_sentiment,np.zeros(len(cluster_labels)))\n",
    "label_graph=[str(list(cluster_labels)[i]) for i in range(len(cluster_labels))]\n",
    "plt.xlim(-9,7)\n",
    "for i,txt in enumerate(label_graph):\n",
    "    ax.annotate(txt,(small_days_sentiment[i],np.zeros(len(cluster_labels))[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORECASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(headlines, stopwords):\n",
    "    '''\n",
    "    Args:\n",
    "    headlines: pd.Series\n",
    "    '''\n",
    "    headlines = headlines.str.lower()\n",
    "    \n",
    "    headlines.replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\$&]','', inplace = True, regex=True)\n",
    "    extensions_list = ['co','reuters','Reuters','bloomberg', 'may','also','could','would', 'na','&amp','gets', 'getting', 'get','must','might','may','across','among','beside','however','yet','within']+list(ascii_lowercase)\n",
    "\n",
    "    stopwords = stopwords.union(set(extensions_list))\n",
    "\n",
    "    wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*headlines.str.split(' ')))))).split(\" \"))\n",
    "\n",
    "    headlines = [' '.join(filter(None,filter(lambda word: word not in stopwords, line))) for line in headlines.str.lower().str.split(' ')]\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_cluster_sentiment(headline, tickers):\n",
    "    '''\n",
    "    Returns the closest cluster to the new headline\n",
    "    \n",
    "    Args:\n",
    "    headline (str)\n",
    "    clusters (dict)\n",
    "    '''\n",
    "    \n",
    "    n_neighors = 5\n",
    "    \n",
    "    # Get SVO of the new headline\n",
    "    cleaned_headline = clean(pd.Series([headline]), stop)\n",
    "    ticker_info = parse_header1(cleaned_headline[0], tickers, None)\n",
    "    \n",
    "    headline_sentiment = {}\n",
    "    \n",
    "    headline_sentiment = {\n",
    "        '3_days': [],\n",
    "        '5_days': [],\n",
    "        '10_days': []\n",
    "    }\n",
    "    \n",
    "    if len(ticker_info) != 0:\n",
    "        try :\n",
    "            for ls in ticker_info:\n",
    "                ticker_key = ls[3]\n",
    "\n",
    "                if ticker_key == 'obj':\n",
    "                    ticker = ls[2]\n",
    "                    svo = [ls[0],ls[1]]\n",
    "                    svo_vec = [np.concatenate((model.wv[ls[1].upper()], model.wv[ls[0].upper()]))]\n",
    "\n",
    "                if ticker_key == 'sub':\n",
    "                    ticker = ls[0]\n",
    "                    svo = [ls[1],ls[2]]\n",
    "                    svo_vec = [np.concatenate((model.wv[ls[1].upper()], model.wv[ls[2].upper()]))]\n",
    "\n",
    "\n",
    "                close_neighbors = clf.kneighbors(svo_vec,n_neighbors)\n",
    "                clusters_list = [labels[close_neighbors[1][0][i]] for i in range(n_neighors)]\n",
    "                distances_list = [close_neighbors[0][0][i] for i in range(n_neighors)]\n",
    "\n",
    "                closest_cluster_label = clusters_list[0]\n",
    "                for horizon in horizons:\n",
    "                    closest_cluster_horizon_sentiment = parsed_sentiments[closest_cluster_label]['{}_days'.format(horizon)]\n",
    "                    headline_sentiment['{}_days'.format(horizon)] = closest_cluster_horizon_sentiment\n",
    "                \n",
    "        except:\n",
    "            return None\n",
    "                \n",
    "    return headline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_distance_sentiment(headline, parsed_sentiments, tickers, labels, horizons):\n",
    "    '''\n",
    "    Returns the closest cluster to the new headline\n",
    "    \n",
    "    Args:\n",
    "    headline (str)\n",
    "    clusters (dict)\n",
    "    tickers (dict): list of company name: ticker pairs\n",
    "    horizons (iterable): iterable of horizon lengths to consider\n",
    "    '''\n",
    "    p = 2\n",
    "    n_neighbours = len(labels)\n",
    "    \n",
    "    # Get SVO of the new headline\n",
    "    cleaned_headline = clean(pd.Series([headline]), stop)\n",
    "    ticker_info = parse_header1(cleaned_headline[0], tickers, None)\n",
    "    \n",
    "    headline_sentiment = {'{}_days'.format(horizon): [] for horizon in horizons}\n",
    "    \n",
    "    if len(ticker_info) == 0:\n",
    "        return None\n",
    "    try :\n",
    "        for ls in ticker_info:\n",
    "            ticker_key = ls[3]\n",
    "\n",
    "            if ticker_key == 'obj':\n",
    "                ticker = ls[2]\n",
    "                svo = [ls[0],ls[1]]\n",
    "                svo_vec = [np.concatenate((model.wv[ls[1].upper()], model.wv[ls[0].upper()]))]\n",
    "\n",
    "            elif ticker_key == 'sub':\n",
    "                ticker = ls[0]\n",
    "                svo = [ls[1],ls[2]]\n",
    "                svo_vec = [np.concatenate((model.wv[ls[1].upper()], model.wv[ls[2].upper()]))]\n",
    "\n",
    "            else:\n",
    "                # Skip if this is a subobj ticker label\n",
    "                continue\n",
    "\n",
    "            neighbours = clf.kneighbors(svo_vec, n_neighbours)\n",
    "            clusters_list = list(set([labels[neighbours[1][0][i]] for i in range(n_neighbours)]) & set(list(parsed_sentiments.keys())))\n",
    "            distances_list = [neighbours[0][0][i] for i in range(n_neighbours)]\n",
    "\n",
    "            new_clusters_list = []\n",
    "            new_distances_list = []\n",
    "\n",
    "            for index, label in enumerate(clusters_list):\n",
    "                if label in parsed_sentiments:\n",
    "                    new_clusters_list.append(clusters_list[index])\n",
    "                    new_distances_list.append(distances_list[index])\n",
    "\n",
    "\n",
    "            distances_list = new_distances_list\n",
    "            clusters_list = new_clusters_list\n",
    "            \n",
    "            #### NOTE! This is not correct\n",
    "\n",
    "            for horizon in horizons:\n",
    "                if 0 in distances_list :\n",
    "                    headline_sentiment['{}_days'.format(horizon)] = parsed_sentiments[clusters_list[0]]['{}_days'.format(horizon)]\n",
    "                else:\n",
    "                    weights = np.array(list(map(lambda x: x**(-p), np.array(distances_list))))\n",
    "                    sentiments = [parsed_sentiments[i]['{}_days'.format(horizon)] for i in clusters_list]\n",
    "                    \n",
    "                    if len(sentiments) == 0:\n",
    "                        print(headline)\n",
    "                    \n",
    "                    if len(weights) == 0:\n",
    "                        print(headline)\n",
    "                        \n",
    "                    headline_sentiment['{}_days'.format(horizon)] = np.average(sentiments, weights = weights)\n",
    "                    \n",
    "                    print(headline_sentiment['{}_days'.format(horizon)])\n",
    "                    \n",
    "                    if np.isnan(headline_sentiment['{}_days'.format(horizon)]):\n",
    "                        print('was nan before')\n",
    "                        headline_sentiment['{}_days'.format(horizon)] = None\n",
    "\n",
    "    except:\n",
    "        print('exception')\n",
    "        return None\n",
    "    \n",
    "    all_empty = True\n",
    "    for val in headline_sentiment.values():\n",
    "        if type(val) == np.float64:\n",
    "            all_empty = False\n",
    "            break\n",
    "            \n",
    "    if all_empty:\n",
    "        print('all empty')\n",
    "        return None\n",
    "\n",
    "    return headline_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_output = []\n",
    "\n",
    "for index in test_data.index:\n",
    "    date = test_data.loc[index, 'Date']\n",
    "    headline = test_data.loc[index, 'Headlines']\n",
    "    \n",
    "    if test_data.loc[index, 'Tick_Type'] == 'obj':\n",
    "        ticker = test_data.loc[index, 'Obj']\n",
    "    if test_data.loc[index, 'Tick_Type'] == 'sub':\n",
    "        ticker = test_data.loc[index, 'Sub']\n",
    "#     else:\n",
    "#         ticker = None\n",
    "    info = {\n",
    "        'date': date,\n",
    "        'headline': headline,\n",
    "        'ticker' : ticker\n",
    "    }\n",
    "    \n",
    "#     horizon_sentiments = inverse_distance_sentiment(\n",
    "#         headline = headline,\n",
    "#         parsed_sentiments = parsed_sentiments,\n",
    "#         tickers = tickers,\n",
    "#         labels = labels,\n",
    "#         horizons = horizons\n",
    "#     )\n",
    "\n",
    "    horizon_sentiments = closest_cluster_sentiment(\n",
    "        headline = headline,\n",
    "        tickers = tickers\n",
    "    )\n",
    "    \n",
    "    if horizon_sentiments is not None:\n",
    "        for horizon in horizons:\n",
    "            info['{}_days'.format(horizon)] = horizon_sentiments['{}_days'.format(horizon)]\n",
    "            \n",
    "        testing_output.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame(testing_output)\n",
    "testing_df\n",
    "\n",
    "# print(len(testing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df[testing_df['headline']=='delta take percent stake latam airlines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in horizons:\n",
    "    for i in testing_df.index:\n",
    "        date = testing_df.loc[i,'date']\n",
    "        ticker = testing_df.loc[i,'ticker']\n",
    "        ticker = ticker.replace('__','')\n",
    "        cumul = lookup_cumul_returns(ticker, date, horizon)\n",
    "        #print(cumul)\n",
    "        testing_df.loc[i,str(horizon)+'_days_actual'] = cumul\n",
    "        \n",
    "testing_df = testing_df.dropna()\n",
    "testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for horizon in horizons:\n",
    "    testing_df[str(horizon)+'_dir_comparison'] = testing_df[str(horizon)+'_days_actual']*testing_df[str(horizon)+'_days']\n",
    "  \n",
    "\n",
    "testing_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_test = testing_df\n",
    "direction_result_dict ={}\n",
    "for horizon in horizons :\n",
    "    pos = len(direction_test.loc[direction_test[str(horizon)+'_dir_comparison']>0,:])\n",
    "    tot = len(direction_test)\n",
    "    direction_result_dict[horizon]=pos/tot\n",
    "    \n",
    "direction_result_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_test = testing_df\n",
    "\n",
    "for horizon in horizons :\n",
    "    magnitude_test[str(horizon)+'_rel_gap']= abs(magnitude_test[str(horizon)+'_days_actual']-magnitude_test[str(horizon)+'_days'])/magnitude_test[str(horizon)+'_days_actual']\n",
    "    \n",
    "magnitude_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_nlp",
   "language": "python",
   "name": "test_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
